# docker-compose.yml
# Complete deployment setup

version: '3.8'

services:
  # Gating Service
  gating-service:
    build:
      context: ./services/gating
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - POSTGRES_URI=${POSTGRES_URI}
      - OLLAMA_URL=${OLLAMA_EXTERNAL_URL}
    restart: unless-stopped

  # Ollama (with LoRA support)
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama-complete
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/models
      - ./scripts:/scripts
    environment:
      - OLLAMA_MODELS=/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  ollama-models:

---
# Dockerfile.ollama-complete
FROM ollama/ollama:latest

# Install Python + dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install training dependencies
RUN pip3 install --no-cache-dir \
    torch==2.1.0 \
    transformers==4.36.0 \
    peft==0.7.0 \
    bitsandbytes==0.41.3 \
    accelerate==0.25.0 \
    datasets==2.15.0 \
    psycopg2-binary==2.9.9 \
    scikit-learn==1.3.2

# Pull base models
RUN nohup ollama serve > /dev/null 2>&1 & \
    sleep 15 && \
    ollama pull nomic-embed-text && \
    ollama pull mistral && \
    pkill ollama

# Create directories
RUN mkdir -p /models/adapters /scripts

# Copy training scripts
COPY scripts/train_complete.py /scripts/train_lora.py
RUN chmod +x /scripts/train_lora.py

EXPOSE 11434

CMD ["serve"]

---
# services/gating/Dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY gating_service.py .

EXPOSE 8080

CMD ["python", "gating_service.py"]

---
# services/gating/requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
psycopg2-binary==2.9.9
pydantic==2.5.0
