#!/usr/bin/env python3
"""
üåç Multilingual Ethical Growth Gating Service - IMPROVED THAI SUPPORT v2
‚úÖ Better Thai keyword detection
‚úÖ Enhanced LLM prompts for Thai language
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional
import re
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import json
import httpx
import logging
import os

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Ethical Growth Gating Service")

# ============================================================
# OLLAMA CONFIGURATION
# ============================================================

OLLAMA_URL = os.getenv("OLLAMA_EXTERNAL_URL", "http://ollama.ollama.svc.cluster.local:11434")
EMBEDDING_MODEL = "nomic-embed-text"  # 768 dimensions
LLM_MODEL = "tinyllama"  # For classification

# ============================================================
# IMPROVED MULTILINGUAL CLASSIFICATION
# ============================================================

async def classify_with_llm(text: str, lang: str) -> Dict:
    """Use Ollama LLM to classify memory with BETTER multilingual support"""
    
    # ‚úÖ IMPROVED: Better Thai examples in prompt
    if lang == 'th':
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏ó‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏° ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: "{text}"

‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:

‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:
- growth_memory: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ï‡∏±‡∏ç‡∏ç‡∏π ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏≤‡∏á‡∏®‡∏≤‡∏™‡∏ô‡∏≤ (‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡πÄ‡∏à‡πâ‡∏≤ ‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤ ‡∏≠‡∏±‡∏•‡∏•‡∏≠‡∏Æ‡πå) ‡∏Å‡∏≤‡∏£‡∏™‡∏ß‡∏î‡∏°‡∏ô‡∏ï‡πå ‡∏ó‡∏≥‡∏ö‡∏∏‡∏ç ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏≤‡∏ö‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
- challenge_memory: ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏ß ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏Å‡∏£‡∏ò ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î‡∏ä‡∏±‡∏á
- wisdom_moment: ‡∏Å‡∏≤‡∏£‡πÑ‡∏ï‡∏£‡πà‡∏ï‡∏£‡∏≠‡∏á‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏¥‡∏î‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ä‡∏ç‡∏≤ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏±‡∏™‡∏£‡∏π‡πâ ‡∏™‡∏ï‡∏¥‡∏õ‡∏±‡∏ç‡∏ç‡∏≤
- needs_support: ‡∏ß‡∏¥‡∏Å‡∏§‡∏ï ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏¥‡πâ‡∏ô‡∏´‡∏ß‡∏±‡∏á ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠
- neutral_interaction: ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏¢‡πÜ

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
- "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡πÄ‡∏à‡πâ‡∏≤" = growth_memory (‡∏®‡∏≤‡∏™‡∏ô‡∏≤/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å)
- "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤" = growth_memory (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ï‡∏±‡∏ç‡∏ç‡∏π/‡∏®‡∏≤‡∏™‡∏ô‡∏≤)
- "‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏°‡∏≤‡∏Å" = growth_memory (‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥)
- "‡∏â‡∏±‡∏ô‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î‡πÄ‡∏Ç‡∏≤" = challenge_memory (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏Å‡∏£‡∏ò)
- "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢" = needs_support (‡∏ß‡∏¥‡∏Å‡∏§‡∏ï)

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):
{{
  "classification": "‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó",
  "self_awareness": 0.0-1.0,
  "emotional_regulation": 0.0-1.0,
  "compassion": 0.0-1.0,
  "integrity": 0.0-1.0,
  "growth_mindset": 0.0-1.0,
  "wisdom": 0.0-1.0,
  "transcendence": 0.0-1.0,
  "reasoning": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
}}"""
    else:
        # English and other languages
        prompt = f"""You are an ethical growth analyst. Analyze this text and respond ONLY with valid JSON.

Text: "{text}"
Language: {lang.upper()}

Classify into ONE category. Consider cultural context:

Categories:
- growth_memory: Positive emotions, gratitude, spiritual/religious growth (God, Buddha, Allah, Jesus), faith, love, learning, appreciation, nature appreciation, kindness
- challenge_memory: Negative emotions, aggression, violence, anger, conflict, harm, hatred
- wisdom_moment: Deep philosophical reflection, insights, enlightenment, meditation, contemplation
- needs_support: Crisis, despair, self-harm thoughts, severe distress, hopelessness
- neutral_interaction: Everyday conversation, neutral statements, factual information

Examples:
- "I love God" = growth_memory (religious/love)
- "Thank you Buddha" = growth_memory (gratitude/religious)
- "Beautiful tree" = growth_memory (nature)
- "I hate them" = challenge_memory (anger)
- "Life is meaningless" = needs_support (crisis)

Respond with ONLY this JSON (no markdown, no explanation):
{{
  "classification": "category_name",
  "self_awareness": 0.0-1.0,
  "emotional_regulation": 0.0-1.0,
  "compassion": 0.0-1.0,
  "integrity": 0.0-1.0,
  "growth_mindset": 0.0-1.0,
  "wisdom": 0.0-1.0,
  "transcendence": 0.0-1.0,
  "reasoning": "brief explanation"
}}"""
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": LLM_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.2,  # Lower for more consistent classification
                        "top_p": 0.9,
                    }
                }
            )
            
            if response.status_code != 200:
                logger.error(f"LLM classification error: {response.status_code}")
                return get_fallback_classification(text, lang)
            
            data = response.json()
            llm_response = data.get("response", "")
            
            logger.info(f"ü§ñ LLM raw response: {llm_response[:200]}")
            
            # Extract JSON from response
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', llm_response)
            if json_match:
                result = json.loads(json_match.group())
                
                # Validate classification
                valid_classifications = [
                    'growth_memory', 'challenge_memory', 'wisdom_moment', 
                    'needs_support', 'neutral_interaction'
                ]
                
                if result.get('classification') not in valid_classifications:
                    logger.warning(f"‚ö†Ô∏è Invalid classification: {result.get('classification')}")
                    result['classification'] = 'neutral_interaction'
                
                # Ensure all scores are present and valid
                for key in ['self_awareness', 'emotional_regulation', 'compassion', 
                           'integrity', 'growth_mindset', 'wisdom', 'transcendence']:
                    if key not in result or not isinstance(result[key], (int, float)):
                        result[key] = 0.5
                    result[key] = max(0.0, min(1.0, float(result[key])))
                
                logger.info(f"‚úÖ LLM classified as: {result['classification']} (lang: {lang})")
                return result
            else:
                logger.warning("‚ö†Ô∏è Could not parse LLM JSON response")
                return get_fallback_classification(text, lang)
                
    except Exception as e:
        logger.error(f"‚ùå LLM classification error: {e}")
        return get_fallback_classification(text, lang)

def get_fallback_classification(text: str, lang: str) -> Dict:
    """‚úÖ IMPROVED: Better Thai keyword detection"""
    text_lower = text.lower()
    
    logger.info(f"üîç Fallback classification for: {text[:50]} (lang: {lang})")
    
    # ‚úÖ ENHANCED Thai keywords
    if lang == 'th':
        growth_keywords = [
            '‡∏£‡∏±‡∏Å', '‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì', '‡∏Å‡∏ï‡∏±‡∏ç‡∏ç‡∏π', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ', '‡∏û‡∏±‡∏í‡∏ô‡∏≤', '‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï',
            '‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡πÄ‡∏à‡πâ‡∏≤', '‡∏û‡∏£‡∏∞', '‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤', '‡∏≠‡∏±‡∏•‡∏•‡∏≠‡∏Æ‡πå', '‡∏ò‡∏£‡∏£‡∏°', '‡∏ö‡∏π‡∏ä‡∏≤', 
            '‡∏™‡∏ß‡∏î‡∏°‡∏ô‡∏ï‡πå', '‡∏ó‡∏≥‡∏ö‡∏∏‡∏ç', '‡πÑ‡∏´‡∏ß‡πâ‡∏û‡∏£‡∏∞', '‡∏®‡∏£‡∏±‡∏ó‡∏ò‡∏≤', '‡πÄ‡∏ä‡∏∑‡πà‡∏≠',
            '‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥', '‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ', '‡∏†‡∏π‡πÄ‡∏Ç‡∏≤', '‡∏ó‡∏∞‡πÄ‡∏•', '‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°', '‡∏á‡∏î‡∏á‡∏≤‡∏°',
            '‡∏ã‡∏≤‡∏ö‡∏ã‡∏∂‡πâ‡∏á', '‡∏î‡∏µ‡∏á‡∏≤‡∏°', '‡πÉ‡∏à‡∏î‡∏µ', '‡πÄ‡∏°‡∏ï‡∏ï‡∏≤', '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤', '‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏à',
            '‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠', '‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô', '‡πÉ‡∏´‡πâ', '‡∏î‡∏µ', '‡∏™‡∏∏‡∏Ç', '‡∏™‡∏±‡∏ô‡∏ï‡∏¥', '‡∏™‡∏á‡∏ö'
        ]
        
        challenge_keywords = [
            '‡∏Ü‡πà‡∏≤', '‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢', '‡πÇ‡∏Å‡∏£‡∏ò', '‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î', '‡∏ä‡∏±‡∏á', '‡∏ó‡∏≥‡∏•‡∏≤‡∏¢', '‡∏£‡πâ‡∏≤‡∏¢',
            '‡πÅ‡∏Å‡πâ‡πÅ‡∏Ñ‡πâ‡∏ô', '‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á', '‡∏ï‡πà‡∏≠‡∏™‡∏π‡πâ', '‡∏ó‡∏∞‡πÄ‡∏•‡∏≤‡∏∞', '‡πÇ‡∏Å‡∏á', '‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á',
            '‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î', '‡∏ó‡∏∏‡∏Å‡∏Ç‡πå', '‡πÄ‡∏®‡∏£‡πâ‡∏≤', '‡πÇ‡∏î‡∏î‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß'
        ]
        
        wisdom_keywords = [
            '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤', '‡∏™‡∏ï‡∏¥', '‡∏™‡∏°‡∏≤‡∏ò‡∏¥', '‡∏ï‡∏£‡∏±‡∏™‡∏£‡∏π‡πâ', '‡∏£‡∏π‡πâ‡πÅ‡∏à‡πâ‡∏á', '‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏à‡πâ‡∏á',
            '‡πÑ‡∏ï‡∏£‡πà‡∏ï‡∏£‡∏≠‡∏á', '‡∏Ñ‡∏¥‡∏î', '‡∏õ‡∏£‡∏±‡∏ä‡∏ç‡∏≤', '‡∏ò‡∏£‡∏£‡∏°‡∏∞', '‡∏ß‡∏¥‡∏õ‡∏±‡∏™‡∏™‡∏ô‡∏≤',
            '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', '‡∏£‡∏π‡πâ', '‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏±‡∏Å', '‡∏´‡∏¢‡∏±‡πà‡∏á‡∏£‡∏π‡πâ'
        ]
        
        support_keywords = [
            '‡∏Ü‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏ï‡∏≤‡∏¢', '‡∏ï‡∏≤‡∏¢', '‡∏™‡∏¥‡πâ‡∏ô‡∏´‡∏ß‡∏±‡∏á', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢', '‡πÑ‡∏£‡πâ‡∏Ñ‡πà‡∏≤',
            '‡∏ó‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', '‡∏¢‡∏≠‡∏°‡πÅ‡∏û‡πâ', '‡∏à‡∏ö‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï', '‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏ß‡∏¢', '‡∏ß‡∏¥‡∏Å‡∏§‡∏ï'
        ]
        
        # Check keywords with logging
        for keyword in growth_keywords:
            if keyword in text:
                logger.info(f"‚úÖ Thai growth keyword found: {keyword}")
                return {
                    'classification': 'growth_memory',
                    'self_awareness': 0.7,
                    'emotional_regulation': 0.6,
                    'compassion': 0.7,
                    'integrity': 0.6,
                    'growth_mindset': 0.7,
                    'wisdom': 0.6,
                    'transcendence': 0.6,
                    'reasoning': f'Thai growth keyword detected: {keyword}'
                }
        
        for keyword in support_keywords:
            if keyword in text:
                logger.info(f"‚ö†Ô∏è Thai support keyword found: {keyword}")
                return {
                    'classification': 'needs_support',
                    'self_awareness': 0.3,
                    'emotional_regulation': 0.2,
                    'compassion': 0.4,
                    'integrity': 0.4,
                    'growth_mindset': 0.3,
                    'wisdom': 0.3,
                    'transcendence': 0.2,
                    'reasoning': f'Thai support keyword detected: {keyword}'
                }
        
        for keyword in challenge_keywords:
            if keyword in text:
                logger.info(f"‚ö†Ô∏è Thai challenge keyword found: {keyword}")
                return {
                    'classification': 'challenge_memory',
                    'self_awareness': 0.3,
                    'emotional_regulation': 0.2,
                    'compassion': 0.4,
                    'integrity': 0.4,
                    'growth_mindset': 0.3,
                    'wisdom': 0.3,
                    'transcendence': 0.2,
                    'reasoning': f'Thai challenge keyword detected: {keyword}'
                }
        
        for keyword in wisdom_keywords:
            if keyword in text:
                logger.info(f"‚úÖ Thai wisdom keyword found: {keyword}")
                return {
                    'classification': 'wisdom_moment',
                    'self_awareness': 0.7,
                    'emotional_regulation': 0.7,
                    'compassion': 0.7,
                    'integrity': 0.7,
                    'growth_mindset': 0.7,
                    'wisdom': 0.8,
                    'transcendence': 0.7,
                    'reasoning': f'Thai wisdom keyword detected: {keyword}'
                }
    
    # English and other languages
    else:
        growth_keywords = ['love', 'thank', 'grateful', 'learn', 'improve', 'grow', 'appreciate', 
                          'god', 'buddha', 'jesus', 'allah', 'prayer', 'worship', 'faith',
                          'nature', 'beautiful', 'tree', 'mountain', 'sea', 'kind', 'help', 'compassion']
        
        challenge_keywords = ['kill', 'murder', 'hurt', 'harm', 'attack', 'hate', 'destroy', 
                             'revenge', 'violent', 'angry', 'rage', 'fight']
        
        wisdom_keywords = ['wisdom', 'insight', 'enlightenment', 'meditation', 'contemplation', 
                          'reflection', 'philosophy', 'truth', 'understanding', 'awareness']
        
        support_keywords = ['suicide', 'die', 'hopeless', 'worthless', 'end it', 'kill myself',
                           'no meaning', 'give up', 'help me', 'crisis']
        
        if any(keyword in text_lower for keyword in growth_keywords):
            logger.info(f"‚úÖ English growth keyword found")
            return {
                'classification': 'growth_memory',
                'self_awareness': 0.7,
                'emotional_regulation': 0.6,
                'compassion': 0.7,
                'integrity': 0.6,
                'growth_mindset': 0.7,
                'wisdom': 0.6,
                'transcendence': 0.6,
                'reasoning': f'Growth keyword detected in {lang}'
            }
        
        if any(keyword in text_lower for keyword in support_keywords):
            return {
                'classification': 'needs_support',
                'self_awareness': 0.3,
                'emotional_regulation': 0.2,
                'compassion': 0.4,
                'integrity': 0.4,
                'growth_mindset': 0.3,
                'wisdom': 0.3,
                'transcendence': 0.2,
                'reasoning': f'Support keyword detected in {lang}'
            }
        
        if any(keyword in text_lower for keyword in challenge_keywords):
            return {
                'classification': 'challenge_memory',
                'self_awareness': 0.3,
                'emotional_regulation': 0.2,
                'compassion': 0.4,
                'integrity': 0.4,
                'growth_mindset': 0.3,
                'wisdom': 0.3,
                'transcendence': 0.2,
                'reasoning': f'Challenge keyword detected in {lang}'
            }
        
        if any(keyword in text_lower for keyword in wisdom_keywords):
            return {
                'classification': 'wisdom_moment',
                'self_awareness': 0.7,
                'emotional_regulation': 0.7,
                'compassion': 0.7,
                'integrity': 0.7,
                'growth_mindset': 0.7,
                'wisdom': 0.8,
                'transcendence': 0.7,
                'reasoning': f'Wisdom keyword detected in {lang}'
            }
    
    # Default neutral
    logger.info(f"‚ÑπÔ∏è No keywords matched, defaulting to neutral")
    return {
        'classification': 'neutral_interaction',
        'self_awareness': 0.5,
        'emotional_regulation': 0.5,
        'compassion': 0.5,
        'integrity': 0.5,
        'growth_mindset': 0.5,
        'wisdom': 0.5,
        'transcendence': 0.3,
        'reasoning': f'Fallback: Neutral classification for {lang}'
    }

# ============================================================
# EMBEDDING GENERATION
# ============================================================

async def generate_embedding(text: str) -> Optional[List[float]]:
    """Generate embedding using Ollama nomic-embed-text"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{OLLAMA_URL}/api/embeddings",
                json={
                    "model": EMBEDDING_MODEL,
                    "prompt": text
                }
            )
            
            if response.status_code != 200:
                logger.error(f"Ollama error: {response.status_code}")
                return None
            
            data = response.json()
            embedding = data.get("embedding")
            
            if not embedding or len(embedding) != 768:
                logger.error(f"Invalid embedding dimension: {len(embedding) if embedding else 0}")
                return None
            
            return embedding
            
    except Exception as e:
        logger.error(f"Embedding generation error: {e}")
        return None

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def detect_language(text: str) -> str:
    """Enhanced language detection"""
    # Thai
    if re.search(r'[\u0E00-\u0E7F]', text):
        return 'th'
    # Chinese
    elif re.search(r'[\u4E00-\u9FFF]', text):
        return 'zh'
    # Japanese
    elif re.search(r'[\u3040-\u309F\u30A0-\u30FF]', text):
        return 'ja'
    # Korean
    elif re.search(r'[\uAC00-\uD7AF]', text):
        return 'ko'
    # Arabic
    elif re.search(r'[\u0600-\u06FF]', text):
        return 'ar'
    else:
        return 'en'

def detect_moments(ethical_scores: Dict, classification: str) -> List[Dict]:
    """Detect significant moments"""
    moments = []
    
    if ethical_scores.get('self_awareness', 0) > 0.7:
        moments.append({
            'type': 'breakthrough',
            'severity': 'positive',
            'description': 'High self-awareness detected',
            'timestamp': datetime.now().isoformat()
        })
    
    if ethical_scores.get('emotional_regulation', 0) < 0.3:
        moments.append({
            'type': 'struggle',
            'severity': 'neutral',
            'description': 'Emotional difficulty detected',
            'timestamp': datetime.now().isoformat()
        })
    
    if classification == 'needs_support':
        moments.append({
            'type': 'crisis',
            'severity': 'critical',
            'description': 'User needs support',
            'timestamp': datetime.now().isoformat(),
            'requires_intervention': True
        })
    
    if classification in ['growth_memory', 'wisdom_moment']:
        moments.append({
            'type': 'growth',
            'severity': 'positive',
            'description': 'Growth or wisdom detected',
            'timestamp': datetime.now().isoformat()
        })
    
    return moments

def determine_growth_stage(ethical_scores: Dict[str, float]) -> int:
    """Determine growth stage"""
    avg_score = sum(ethical_scores.values()) / len(ethical_scores)
    
    if avg_score < 0.3:
        return 1
    elif avg_score < 0.5:
        return 2
    elif avg_score < 0.7:
        return 3
    elif avg_score < 0.85:
        return 4
    else:
        return 5

# ============================================================
# DATABASE OPERATIONS
# ============================================================

def save_ethical_profile(user_id: str, ethical_scores: Dict, stage: int, db_conn):
    cursor = db_conn.cursor()
    
    cursor.execute("""
        INSERT INTO user_data_schema.ethical_profiles 
        (user_id, self_awareness, emotional_regulation, compassion, 
         integrity, growth_mindset, wisdom, transcendence, growth_stage, updated_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
        ON CONFLICT (user_id) 
        DO UPDATE SET
            self_awareness = EXCLUDED.self_awareness,
            emotional_regulation = EXCLUDED.emotional_regulation,
            compassion = EXCLUDED.compassion,
            integrity = EXCLUDED.integrity,
            growth_mindset = EXCLUDED.growth_mindset,
            wisdom = EXCLUDED.wisdom,
            transcendence = EXCLUDED.transcendence,
            growth_stage = EXCLUDED.growth_stage,
            total_interactions = ethical_profiles.total_interactions + 1,
            updated_at = NOW()
    """, (
        user_id,
        ethical_scores['self_awareness'],
        ethical_scores['emotional_regulation'],
        ethical_scores['compassion'],
        ethical_scores['integrity'],
        ethical_scores['growth_mindset'],
        ethical_scores['wisdom'],
        ethical_scores['transcendence'],
        stage
    ))
    
    db_conn.commit()
    cursor.close()

async def save_memory_with_embedding(
    user_id: str, 
    text: str,
    embedding: List[float],
    classification: str,
    lang: str,
    growth_stage: int,
    db_conn
) -> str:
    """Save to memory_embeddings"""
    cursor = db_conn.cursor()
    
    vector_str = f"[{','.join(map(str, embedding))}]"
    
    metadata = {
        'classification': classification,
        'language': lang,
        'growth_stage': growth_stage,
        'source': 'gating_service',
        'created_at': datetime.now().isoformat()
    }
    
    cursor.execute("""
        INSERT INTO user_data_schema.memory_embeddings
        (user_id, content, embedding, metadata, created_at)
        VALUES (%s, %s, %s::vector, %s, NOW())
        RETURNING id
    """, (
        user_id,
        text,
        vector_str,
        json.dumps(metadata)
    ))
    
    memory_id = cursor.fetchone()[0]
    db_conn.commit()
    cursor.close()
    
    logger.info(f"‚úÖ Memory saved with ID: {memory_id}")
    return str(memory_id)

def save_interaction_memory(
    user_id: str, 
    text: str, 
    classification: str,
    ethical_scores: Dict,
    moments: List[Dict],
    reflection_prompt: str,
    gentle_guidance: Optional[str],
    memory_embedding_id: str,
    db_conn
):
    """Save to interaction_memories"""
    cursor = db_conn.cursor()
    
    cursor.execute("""
        INSERT INTO user_data_schema.interaction_memories
        (user_id, text, classification, ethical_scores, moments, 
         reflection_prompt, gentle_guidance, metadata, created_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
        RETURNING id
    """, (
        user_id,
        text,
        classification,
        json.dumps(ethical_scores),
        json.dumps(moments),
        reflection_prompt,
        gentle_guidance,
        json.dumps({
            'source': 'gating_service',
            'memory_embedding_id': memory_embedding_id
        })
    ))
    
    db_conn.commit()
    cursor.close()
    logger.info(f"‚úÖ Interaction memory saved")

# ============================================================
# GUIDANCE TEMPLATES
# ============================================================

GUIDANCE_TEMPLATES = {
    'crisis': {
        'en': "I'm concerned about you. Please reach out to a mental health professional.",
        'th': "‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏™‡∏≤‡∏¢‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï 1323",
    },
    'emotional_dysregulation': {
        'en': "Take a deep breath. These feelings will pass.",
        'th': "‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡πÄ‡∏Ç‡πâ‡∏≤‡∏•‡∏∂‡∏Å‡πÜ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ",
    },
}

REFLECTION_PROMPTS = {
    1: {
        'en': "What are you feeling right now?",
        'th': "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
    },
    2: {
        'en': "If someone else were in this situation, how would they feel?",
        'th': "‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ ‡πÄ‡∏Ç‡∏≤‡∏à‡∏∞‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏á?",
    },
    3: {
        'en': "What values does this decision reflect?",
        'th': "‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ô‡∏µ‡πâ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£?",
    },
}

def get_guidance(classification: str, ethical_scores: Dict, lang: str) -> Optional[str]:
    if classification == 'needs_support':
        return GUIDANCE_TEMPLATES['crisis'].get(lang, GUIDANCE_TEMPLATES['crisis']['en'])
    
    if ethical_scores.get('emotional_regulation', 0.5) < 0.3:
        return GUIDANCE_TEMPLATES['emotional_dysregulation'].get(lang, GUIDANCE_TEMPLATES['emotional_dysregulation']['en'])
    
    return None

def get_reflection_prompt(stage: int, lang: str) -> str:
    prompts = REFLECTION_PROMPTS.get(stage, REFLECTION_PROMPTS[2])
    return prompts.get(lang, prompts.get('en', ''))

# ============================================================
# API MODELS
# ============================================================

class GatingRequest(BaseModel):
    user_id: str
    text: str
    database_url: str
    session_id: Optional[str] = None
    metadata: Optional[Dict] = {}

class GatingResponse(BaseModel):
    status: str
    routing: str
    ethical_scores: Dict[str, float]
    growth_stage: int
    moments: List[Dict]
    insights: Optional[Dict] = None
    reflection_prompt: Optional[str] = None
    gentle_guidance: Optional[str] = None
    growth_opportunity: Optional[str] = None
    detected_language: Optional[str] = None
    memory_id: Optional[str] = None

# ============================================================
# MAIN ENDPOINT
# ============================================================

@app.post("/gating/ethical-route", response_model=GatingResponse)
async def ethical_routing(request: GatingRequest):
    """Process text with improved Thai support"""
    
    logger.info(f"üìù Processing text for user {request.user_id}: {request.text[:50]}...")
    
    if not request.database_url:
        raise HTTPException(status_code=400, detail="database_url is required")
    
    db_conn = psycopg2.connect(request.database_url)
    
    try:
        # 1. Detect language
        lang = detect_language(request.text)
        logger.info(f"üåç Detected language: {lang}")
        
        # 2. Generate embedding
        logger.info(f"üß† Generating embedding...")
        embedding = await generate_embedding(request.text)
        
        if not embedding:
            logger.warning("‚ö†Ô∏è Embedding generation failed")
        
        # 3. LLM CLASSIFICATION with fallback
        logger.info(f"ü§ñ Using LLM for classification (language: {lang})...")
        llm_result = await classify_with_llm(request.text, lang)
        
        classification = llm_result['classification']
        ethical_scores = {
            'self_awareness': llm_result['self_awareness'],
            'emotional_regulation': llm_result['emotional_regulation'],
            'compassion': llm_result['compassion'],
            'integrity': llm_result['integrity'],
            'growth_mindset': llm_result['growth_mindset'],
            'wisdom': llm_result['wisdom'],
            'transcendence': llm_result['transcendence'],
        }
        
        logger.info(f"‚úÖ Classification: {classification}")
        logger.info(f"üìä Reasoning: {llm_result.get('reasoning', 'N/A')}")
        
        # 4. Determine growth stage
        growth_stage = determine_growth_stage(ethical_scores)
        
        # 5. Detect moments
        moments = detect_moments(ethical_scores, classification)
        
        # 6. Generate guidance
        reflection_prompt = get_reflection_prompt(growth_stage, lang)
        gentle_guidance = get_guidance(classification, ethical_scores, lang)
        
        # 7. Save to memory_embeddings
        memory_id = None
        if embedding:
            logger.info(f"üíæ Saving to memory_embeddings...")
            memory_id = await save_memory_with_embedding(
                request.user_id,
                request.text,
                embedding,
                classification,
                lang,
                growth_stage,
                db_conn
            )
        else:
            logger.error("‚ùå Cannot save without embedding")
            raise HTTPException(status_code=500, detail="Embedding generation failed")
        
        # 8. Save ethical profile
        save_ethical_profile(request.user_id, ethical_scores, growth_stage, db_conn)
        
        # 9. Save interaction memory
        save_interaction_memory(
            request.user_id,
            request.text,
            classification,
            ethical_scores,
            moments,
            reflection_prompt,
            gentle_guidance,
            memory_id,
            db_conn
        )
        
        logger.info(f"‚úÖ Processing completed: {classification}")
        
        return GatingResponse(
            status='success',
            routing=classification,
            ethical_scores=ethical_scores,
            growth_stage=growth_stage,
            moments=moments,
            insights={
                'strongest_dimension': max(ethical_scores, key=ethical_scores.get),
                'growth_area': min(ethical_scores, key=ethical_scores.get),
                'llm_reasoning': llm_result.get('reasoning', 'N/A')
            },
            reflection_prompt=reflection_prompt,
            gentle_guidance=gentle_guidance,
            growth_opportunity=f"Stage {growth_stage}/5",
            detected_language=lang,
            memory_id=memory_id
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db_conn.close()

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "service": "ethical_growth_gating",
        "version": "4.0-improved-thai",
        "supported_languages": ["en", "th", "zh", "ja", "ko", "ar", "and more"],
        "multilingual": True,
        "embedding_model": EMBEDDING_MODEL,
        "classification_model": LLM_MODEL,
        "ollama_url": OLLAMA_URL
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
    
